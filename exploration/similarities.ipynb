{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download de_core_news_sm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.matutils import sparse2full\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Information**\n",
    "\n",
    "Prefer spacy instead of nltk because it's faster and we want to compute large texts.\n",
    "Lemmatization and Stemming are redundand and different aproaches. They normaly are not used together. Decided for Lemmatizing because its integrated into spacy module:\n",
    "\n",
    "\n",
    "Sources:\n",
    "- Text Similarity Measures in News Articles by Vector Space Model Using NLP (https://link.springer.com/article/10.1007/s40031-020-00501-5)\n",
    "- Compare documents similarity using Python | NLP (https://dev.to/thedevtimeline/compare-documents-similarity-using-python-nlp-4odp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Das ist ein Text.\",\n",
    "    \"Ich habe einen Text geschrieben.\",\n",
    "    \"Ich habe mehrere Texte geschrieben!\",\n",
    "    \"Das sind viele texte. Insgesamt sind es 4.\"\n",
    "]\n",
    "\n",
    "sp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Documents:\n",
      "['Das', 'ist', 'ein', 'Text', '.']\n",
      "['Ich', 'habe', 'einen', 'Text', 'geschrieben', '.']\n",
      "['Ich', 'habe', 'mehrere', 'Texte', 'geschrieben', '!']\n",
      "['Das', 'sind', 'viele', 'texte', '.', 'Insgesamt', 'sind', 'es', '4.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    doc_tokens = sp(doc)\n",
    "    tokenized_tokens = [token.text for token in doc_tokens]\n",
    "    tokenized_docs.append(tokenized_tokens)\n",
    "\n",
    "print(\"Tokenized Documents:\")\n",
    "for doc_tokens in tokenized_docs:\n",
    "    print(doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Lammatize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents:\n",
      "['der', 'sein', 'ein', 'Text', '--']\n",
      "['ich', 'haben', 'ein', 'Text', 'schreiben', '--']\n",
      "['ich', 'haben', 'mehrere', 'Text', 'schreiben', '--']\n",
      "['der', 'sein', 'vieler', 'Text', '--', 'insgesamt', 'sein', 'es', '4.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs = []\n",
    "for doc_tokens in tokenized_docs:\n",
    "    lemmatized_tokens = [token.lemma_ for token in sp(' '.join(doc_tokens))]\n",
    "    lemmatized_docs.append(lemmatized_tokens)\n",
    "\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Remove Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents without Stop Words:\n",
      "['Text', '--']\n",
      "['Text', 'schreiben', '--']\n",
      "['mehrere', 'Text', 'schreiben', '--']\n",
      "['vieler', 'Text', '--', 'insgesamt', '4.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs_no_stopwords = []\n",
    "\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not sp.vocab[token].is_stop]\n",
    "    lemmatized_docs_no_stopwords.append(lemmatized_tokens_no_stopwords)\n",
    "\n",
    "print(\"\\nLemmatized Documents without Stop Words:\")\n",
    "for lemmatized_tokens in lemmatized_docs_no_stopwords:\n",
    "    print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.Eleminate Puctation Marks & Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents without Stop Words, Punctuation, and Numbers:\n",
      "['Text']\n",
      "['Text', 'schreiben']\n",
      "['mehrere', 'Text', 'schreiben']\n",
      "['vieler', 'Text', 'insgesamt']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs_no_stopwords_punct_nums = []\n",
    "\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not sp.vocab[token].is_stop]\n",
    "    \n",
    "    # Remove punctuation tokens\n",
    "    lemmatized_tokens_no_punct = [token for token in lemmatized_tokens_no_stopwords if not sp.vocab[token].is_punct]\n",
    "    \n",
    "    # Remove number tokens\n",
    "    lemmatized_tokens_no_nums = [token for token in lemmatized_tokens_no_punct if not sp.vocab[token].like_num]\n",
    "    \n",
    "    lemmatized_docs_no_stopwords_punct_nums.append(lemmatized_tokens_no_nums)\n",
    "\n",
    "print(\"\\nLemmatized Documents without Stop Words, Punctuation, and Numbers:\")\n",
    "for tokens in lemmatized_docs_no_stopwords_punct_nums:\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Documents:\n",
      "['Text']\n",
      "['Text', 'schreiben']\n",
      "['mehrere', 'Text', 'schreiben']\n",
      "['vieler', 'Text', 'insgesamt']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"de_core_news_sm\")\n",
    "    \n",
    "    def tokenize_documents(self, documents):\n",
    "        tokenized_docs = []\n",
    "        for doc in documents:\n",
    "            doc_tokens = self.nlp(doc)\n",
    "            tokenized_tokens = [token.text for token in doc_tokens]\n",
    "            tokenized_docs.append(tokenized_tokens)\n",
    "        return tokenized_docs\n",
    "    \n",
    "    def lemmatize_documents(self, tokenized_docs):\n",
    "        lemmatized_docs = []\n",
    "        for doc_tokens in tokenized_docs:\n",
    "            lemmatized_tokens = [token.lemma_ for token in self.nlp(' '.join(doc_tokens))]\n",
    "            lemmatized_docs.append(lemmatized_tokens)\n",
    "        return lemmatized_docs\n",
    "    \n",
    "    def remove_stopwords_punctuations_numbers(self, lemmatized_docs):\n",
    "        clean_docs = []\n",
    "        for lemmatized_tokens in lemmatized_docs:\n",
    "            lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not self.nlp.vocab[token].is_stop]\n",
    "            lemmatized_tokens_no_punct = [token for token in lemmatized_tokens_no_stopwords if not self.nlp.vocab[token].is_punct]\n",
    "            lemmatized_tokens_no_nums = [token for token in lemmatized_tokens_no_punct if not self.nlp.vocab[token].like_num]\n",
    "            clean_docs.append(lemmatized_tokens_no_nums)\n",
    "        return clean_docs\n",
    "\n",
    "documents = [\n",
    "    \"Das ist ein Text.\",\n",
    "    \"Ich habe einen Text geschrieben.\",\n",
    "    \"Ich habe mehrere Texte geschrieben!\",\n",
    "    \"Das sind viele texte. Insgesamt sind es 4.\"\n",
    "]\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Tokenization\n",
    "tokenized = preprocessor.tokenize_documents(documents)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized = preprocessor.lemmatize_documents(tokenized)\n",
    "\n",
    "# Removing Stopwords, Punctuation, and Numbers\n",
    "cleaned = preprocessor.remove_stopwords_punctuations_numbers(lemmatized)\n",
    "\n",
    "# Print cleaned documents\n",
    "print(\"\\nCleaned Documents:\")\n",
    "for tokens in cleaned:\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Representation Scheme**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag-of-Words Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>schreiben</th>\n",
       "      <th>mehrere</th>\n",
       "      <th>insgesamt</th>\n",
       "      <th>vieler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Text  schreiben  mehrere  insgesamt  vieler\n",
       "0   1.0        0.0      0.0        0.0     0.0\n",
       "1   1.0        1.0      0.0        0.0     0.0\n",
       "2   1.0        1.0      1.0        0.0     0.0\n",
       "3   1.0        0.0      0.0        1.0     1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<5 unique tokens: ['Text', 'schreiben', 'mehrere', 'insgesamt', 'vieler']>\n",
      "[[(0, 1)], [(0, 1), (1, 1)], [(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1)]]\n"
     ]
    }
   ],
   "source": [
    "def create_bow_representation(preprocessed_docs):\n",
    "    dictionary = corpora.Dictionary(preprocessed_docs)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "    return dictionary, bow_corpus\n",
    "\n",
    "bow_dictionary, bow_corpus = create_bow_representation(cleaned)\n",
    "\n",
    "# Display bow representation\n",
    "print(\"\\nBag-of-Words Representation:\")\n",
    "num_terms = len(bow_dictionary)\n",
    "bow_matrix = [sparse2full(doc, num_terms) for doc in bow_corpus]\n",
    "bow_df = pd.DataFrame(bow_matrix, columns=[bow_dictionary[i] for i in range(num_terms)])\n",
    "\n",
    "display(bow_df)\n",
    "\n",
    "print(bow_dictionary)\n",
    "print(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\USERNA~1\\AppData\\Local\\Temp/ipykernel_4936/2471273606.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_corpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtfidf_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_tfidf_representation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbow_corpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \"\"\"\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "def create_tfidf_representation(preprocessed_docs):\n",
    "    dictionary = corpora.Dictionary(preprocessed_docs)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "    \n",
    "    # Generate TF-IDF model based on the bag-of-words corpus\n",
    "    tfidf_model = models.TfidfModel(bow_corpus)\n",
    "    \n",
    "    # Transform the bag-of-words corpus to TF-IDF corpus\n",
    "    tfidf_corpus = tfidf_model[bow_corpus]\n",
    "    \n",
    "    return tfidf_model, tfidf_corpus, dictionary\n",
    "\n",
    "# Create TF-IDF representation\n",
    "tfidf_model, tfidf_corpus, tfidf_dictionary = create_tfidf_representation(cleaned)\n",
    "\n",
    "# Display TF-IDF representation\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "num_terms = len(tfidf_dictionary)\n",
    "tfidf_matrix = [sparse2full(doc, num_terms) for doc in tfidf_corpus]\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix, columns=[tfidf_dictionary[i] for i in range(num_terms)])\n",
    "\n",
    "display(tfidf_df)\n",
    "\n",
    "# Display TF-IDF dictionary and corpus\n",
    "print(\"\\nTF-IDF Dictionary:\")\n",
    "print(tfidf_dictionary)\n",
    "\n",
    "print(\"\\nTF-IDF Corpus:\")\n",
    "print(tfidf_corpus)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Similarity Measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cosine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity (BoW Representation):\n",
      "MatrixSimilarity<4 docs, 5 features>\n",
      "Document 1: [1.         0.70710677 0.57735026 0.57735026]\n",
      "Document 2: [0.70710677 0.99999994 0.81649655 0.40824828]\n",
      "Document 3: [0.57735026 0.81649655 0.99999994 0.3333333 ]\n",
      "Document 4: [0.57735026 0.40824828 0.3333333  0.99999994]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# Calculate cosine similarity for TF-IDF representation\\ntfidf_similarity_index = calculate_cosine_similarity(tfidf_corpus, similarity_type=\\'tfidf\\')\\nprint(\"\\nCosine Similarity (TF-IDF Representation):\")\\nprint(tfidf_similarity_index)\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_cosine_similarity(corpus, similarity_type='bow'):\n",
    "    if similarity_type == 'bow':\n",
    "        index = similarities.MatrixSimilarity(corpus)\n",
    "    elif similarity_type == 'tfidf':\n",
    "        index = similarities.MatrixSimilarity(corpus, num_features=len(corpus))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid similarity_type. Choose 'bow' or 'tfidf'.\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "bow_similarity_index = calculate_cosine_similarity(bow_corpus, similarity_type='bow')\n",
    "print(\"\\nCosine Similarity (BoW Representation):\")\n",
    "print(bow_similarity_index)\n",
    "for i, sims in enumerate(bow_similarity_index):\n",
    "    print(f\"Document {i + 1}: {sims}\")\n",
    "\n",
    "\"\"\"\n",
    "# Calculate cosine similarity for TF-IDF representation\n",
    "tfidf_similarity_index = calculate_cosine_similarity(tfidf_corpus, similarity_type='tfidf')\n",
    "print(\"\\nCosine Similarity (TF-IDF Representation):\")\n",
    "print(tfidf_similarity_index)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
