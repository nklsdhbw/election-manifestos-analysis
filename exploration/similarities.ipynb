{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download de_core_news_sm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, similarities\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Information**\n",
    "\n",
    "Prefer spacy instead of nltk because it's faster and we want to compute large texts.\n",
    "Lemmatization and Stemming are redundand and different aproaches. They normaly are not used together. Decided for Lemmatizing because its integrated into spacy module:\n",
    "\n",
    "\n",
    "Sources:\n",
    "- Text Similarity Measures in News Articles by Vector Space Model Using NLP (https://link.springer.com/article/10.1007/s40031-020-00501-5)\n",
    "- Compare documents similarity using Python | NLP (https://dev.to/thedevtimeline/compare-documents-similarity-using-python-nlp-4odp)\n",
    "- What is gensim.similarities.MatrixSimilarity() function? (https://www.educative.io/answers/what-is-gensimsimilaritiesmatrixsimilarity-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Das ist ein Text.\",\n",
    "    \"Ich habe einen Text geschrieben.\",\n",
    "    \"Ich habe mehrere Texte geschrieben!\",\n",
    "    \"Das sind viele texte. Insgesamt sind es 4.\"\n",
    "]\n",
    "\n",
    "sp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Documents:\n",
      "['Das', 'ist', 'ein', 'Text', '.']\n",
      "['Ich', 'habe', 'einen', 'Text', 'geschrieben', '.']\n",
      "['Ich', 'habe', 'mehrere', 'Texte', 'geschrieben', '!']\n",
      "['Das', 'sind', 'viele', 'texte', '.', 'Insgesamt', 'sind', 'es', '4.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    doc_tokens = sp(doc)\n",
    "    tokenized_tokens = [token.text for token in doc_tokens]\n",
    "    tokenized_docs.append(tokenized_tokens)\n",
    "\n",
    "print(\"Tokenized Documents:\")\n",
    "for doc_tokens in tokenized_docs:\n",
    "    print(doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Lammatize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents:\n",
      "['der', 'sein', 'ein', 'Text', '--']\n",
      "['ich', 'haben', 'ein', 'Text', 'schreiben', '--']\n",
      "['ich', 'haben', 'mehrere', 'Text', 'schreiben', '--']\n",
      "['der', 'sein', 'vieler', 'Text', '--', 'insgesamt', 'sein', 'es', '4.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs = []\n",
    "for doc_tokens in tokenized_docs:\n",
    "    lemmatized_tokens = [token.lemma_ for token in sp(' '.join(doc_tokens))]\n",
    "    lemmatized_docs.append(lemmatized_tokens)\n",
    "\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Remove Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents without Stop Words:\n",
      "['Text', '--']\n",
      "['Text', 'schreiben', '--']\n",
      "['mehrere', 'Text', 'schreiben', '--']\n",
      "['vieler', 'Text', '--', 'insgesamt', '4.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs_no_stopwords = []\n",
    "\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not sp.vocab[token].is_stop]\n",
    "    lemmatized_docs_no_stopwords.append(lemmatized_tokens_no_stopwords)\n",
    "\n",
    "print(\"\\nLemmatized Documents without Stop Words:\")\n",
    "for lemmatized_tokens in lemmatized_docs_no_stopwords:\n",
    "    print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.Eleminate Puctation Marks & Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents without Stop Words, Punctuation, and Numbers:\n",
      "['Text']\n",
      "['Text', 'schreiben']\n",
      "['mehrere', 'Text', 'schreiben']\n",
      "['vieler', 'Text', 'insgesamt']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs_no_stopwords_punct_nums = []\n",
    "\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not sp.vocab[token].is_stop]\n",
    "    \n",
    "    # Remove punctuation tokens\n",
    "    lemmatized_tokens_no_punct = [token for token in lemmatized_tokens_no_stopwords if not sp.vocab[token].is_punct]\n",
    "    \n",
    "    # Remove number tokens\n",
    "    lemmatized_tokens_no_nums = [token for token in lemmatized_tokens_no_punct if not sp.vocab[token].like_num]\n",
    "    \n",
    "    lemmatized_docs_no_stopwords_punct_nums.append(lemmatized_tokens_no_nums)\n",
    "\n",
    "print(\"\\nLemmatized Documents without Stop Words, Punctuation, and Numbers:\")\n",
    "for tokens in lemmatized_docs_no_stopwords_punct_nums:\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Documents:\n",
      "['unions-Versprechen', 'arbeiten', 'modern', 'Europa', 'weltpolitikfähig', '\\n ', 'global', 'Herausforderung', 'gemeinsam', 'meistern', 'Europa', 'handlungsfähig', '\\n ', 'mutig', 'entschlossen', 'Europa', 'Deutschland', '\\n ', 'Europa', 'herausfordern', 'innen', 'außen', 'innerhalb', 'Europa', 'setzen', 'Populist', '\\n ', 'links', 'rechts', 'europäisch', 'Demokratie', 'Druck', 'zusätzlich', 'erschweren', 'Nationalismus', 'Eigeninteresse', 'EU-Mitgliedsstaat', 'gemeinsam', 'europäisch', '\\n ', 'Lösung', 'verhindern', 'auftreten', 'EU', 'Stimme', 'schließlich', 'EU', '\\n ', 'zentral', 'Bereich', 'Verteidigungspolitik', 'handlungsfähig', '\\n ', 'wünschen', '\\n ', 'außen', 'sehen', 'europäisch-abendländisch', 'Leitgedanke', 'Demokratie', '\\n ', 'sozial', 'Marktwirtschaft', 'Druck', 'Wettbewerb', 'konkurrierend', 'Gesellschaft', 'Wirtschaftsmodelle', 'frei', 'Welthandel', 'Markt', 'Wohlstand', 'bringen', 'Selbstverständlichkeit', '\\n ', 'Antwort', 'Herausforderung', 'lauten', 'Europa', 'gemeinsam', 'europäisch', 'Partner', 'Herausforderung', 'meistern', 'brauchen', '\\n ', 'schnell', 'dynamischerer', 'Entscheidung', 'europäisch', 'Lösung', 'entschlossen', 'handeln', 'international', 'Ebene', 'brauchen', 'gelten', 'Formel', 'Problem', '\\n ', 'Europa', 'Problem', 'Europa', 'gleichzeitig', 'Modernisierungsjahrzehnt', '\\n ', 'Europa', 'erstrecken', 'investieren', 'Technologie', 'Innovation', 'Europa', 'Wirtschaft', 'Zukunft', 'Garant', 'Wohlstand', 'Arbeitsplatz', 'Nachhaltigkeit', 'bleiben', 'investieren', 'Europa', 'Sicherheit', 'innen', 'außen', 'Kind', 'Enkel', '\\n ', 'Europa', 'Frieden', 'Freiheit', 'Sicherheit', 'leben']\n",
      "['AfD', 'stehen', 'Freiheit', 'Selbstbestimmung', '\\n ', 'europäisch', 'Nation', 'bekennen', '\\n ', 'Europa', 'Vaterländ', 'Gemeinschaft', '\\n ', 'souverän', 'Staat', 'all', 'Gebiet', '\\n ', 'zusammenarbeiten', 'gemeinsam', 'gestalten', '\\n ', 'gehören', 'insbesondere', 'frei', '\\n ', 'Handel', 'fair', 'Wettbewerb', '\\n ', 'staatsähnlich', 'europäisch', 'Union', '\\n ', 'etabliert', 'Partei', 'anstreben', 'halten', 'Sinn', '\\n ', 'prosperierend', 'friedlich', 'Europa', '\\n ', 'kontraproduktiv', 'selbstverantwortlich', '\\n ', 'lebendig', 'Demokratie', 'gestaltet', 'Nationalstaat', '\\n ', 'übernational', 'Einrichtung', 'ersetzbar', '\\n ', 'Gruppe', 'benachbart', 'Staat', '\\n ', 'völkerrechtlich', 'Basis', 'konstruktiv', 'friedlich', '\\n ', 'kooperieren', 'Versuch', 'derzeit', '\\n ', 'Staat', 'jeweils', 'Sprache', '\\n ', 'Kultur', 'historisch', 'Erfahrung', '\\n ', 'ausgestaltet', 'Gesamtstaat', 'bilden', '\\n ', 'scheitern', 'Gebilde', 'verfügen', 'weder', '\\n ', 'Staatsvolk', 'erforderlich', 'Mindestmaß', '\\n ', 'kulturell', 'Identität', 'notwendig', 'Voraussetzung', 'gelingend', 'Staat', '\\n ', 'souverän', 'demokratisch', 'Nationalstaat', 'erhalten', '\\n ', 'Volkssouveränität', 'leben', '\\n ', 'Mutter', 'Herzstück', 'Demokratie', 'Union', 'europäisch', 'Staat', '\\n ', 'Zukunft', 'gelingen', '\\n ', 'schnell', 'drehend', 'Rad', 'Entdemokratisierung', '\\n ', 'Zentralisierung', 'Speiche', 'greifen', 'bevor', '\\n ', 'heutig', 'EU', 'Pervertierung', 'Gründungsidee', 'zugrunde', 'wirtschaftsgeschichtlich', 'ungewöhnlich', 'Idee', 'Einheitswährung', 'wirtschaftlich', 'völlig', 'unterschiedlich', '\\n ', 'entwickelt', 'Staat', 'scheitern', 'handeln', '\\n ', 'politisch', 'Wunschvorstellung', '\\n ', 'ökonomisch', 'Gesetz', 'Einklang', 'bringen', '\\n ', 'Einführung', 'sogenannter', 'CoronaWiederaufbaupakt', 'Transferunion', '\\n ', 'neu', 'Dimension', 'heben', 'Transferunion', 'stehen', '\\n ', 'Widerspruch', 'europäisch', '\\n ', 'Vertrag', 'Versprechen', 'deutsch', 'Politiker', '\\n ', 'Abstieg', 'europäisch', '\\n ', 'Volkswirtschaft', 'Konflikt', 'Staat', '\\n ', 'Folge', '\\n ', 'vergleichbar', 'versagen', 'zeigen', 'EU', 'jahrelang', '\\n ', 'anhaltend', 'Migrationskrise', 'Krisenmanagement', 'Corona-Krise', 'verheerend', '\\n ', 'Fehlentwicklung', 'Klima', 'Energiepolitik', '\\n ', 'treiben', 'EU', 'unvorstellbar', 'kostspielig', 'Gesetzesund', 'Subventionspakete', 'voran', 'langfristig', 'Unheil', '\\n ', 'Verschuldung', 'Umverteilungs-Eskapade', '\\n ', 'deutsch', 'Steuerzahler', 'treffen', 'Vehemenz', 'europäisch', 'Union', '\\n ', 'Transformation', 'planwirtschaftlich', 'Superstaat', '\\n ', 'letzter', 'vorantreiben', '\\n ', 'Erkenntnis', 'bringen', 'grundlegend', '\\n ', 'Reformansatz', 'EU', 'verwirklichen', 'lassen', '\\n ', 'halten', 'Austritt', 'Deutschland', '\\n ', 'europäisch', 'Union', 'Gründung', 'neu', '\\n ', 'europäisch', 'Wirtschaft', 'Interessengemeinschaft', '\\n ', 'notwendig']\n",
      "['grenzenlos', 'reisen', 'arbeiten', 'studieren', 'leben', 'europäisch', 'Union', 'EU', 'Leben', '\\n ', 'Million', 'Mensch', 'prägen', 'neu', 'Möglichkeit', 'Freiheit', 'eröffnen', 'unermesslich', 'Wert', 'kulturell', 'Vielfalt', 'Gesellschaft', 'erlebbar', 'zeigen', 'gemeinsam', 'erreichen', 'Zukunft', '\\n ', 'ankommen', '\\n ', 'global', 'Wettbewerb', 'geprägt', 'Umfeld', 'europäisch', 'Wert', '\\n ', 'Interesse', 'behaupten', 'Europa', 'innen', 'außen', 'handlungsfähig', '\\n ', 'Freiheit', 'Rechtstaatlichkeit', 'Europa', 'schützen', 'EU', 'modern', '\\n ', 'Demokratie', 'Welt', 'Europa', 'Klimaschutz', 'Vorreiter', '\\n ', 'digital', 'souverän', 'Europa', 'Basis', 'wertebasiert', 'digital', 'Wirtschaft', '\\n ', 'Investition', 'gemeinsam', 'Wirtschaft', 'Innovationskraft', 'stärken', 'Europa', '\\n ', 'modern', 'Sozialst', 'nachhaltigsten', 'wettbewerbsfähigst', 'Wirtschaftsraum', 'Welt', '\\n ', 'sichern', 'Grundlage', 'Wohlstand', 'schaffen', 'Voraussetzung', '\\n ', 'souverän', 'Europa', 'sozial', 'Gerechtigkeit', 'Wohlstand', 'Menschenrecht', 'stehen', '\\n ', 'schließen', 'gerecht', 'friedlicher', 'nachhaltig', 'Welt', 'einsetzen', '\\n ', 'miteinander', 'humanitär', 'solidarisch', 'Flüchtlingspolitik', 'gewährleisten', '\\n ', 'Demokratie', 'Freiheit', 'Rechtsstaatlichkeit', 'Fundament', 'Gemeinschaft', 'zulassen', 'nationalistisch', 'Hass', 'populistisch', 'Hetze', 'Europa', 'spalten', '\\n ', 'ernst', 'europäisch', 'Solidarität', 'Beginn', 'Pandemie', 'schnell', '\\n ', 'kraftvoll', 'handeln', 'Wiederaufbaufond', 'Geschichte', 'europäisch', '\\n ', 'Union', 'Weg', 'bringen', 'Solidarleistung', 'sozial', 'Folge', 'Corona-Krise', 'abmilderen', 'gleichzeitig', 'sozial-ökologisch', 'Wandel', 'vorantreiben', 'Innovation', 'fördern', '\\n ', 'Europa', 'bekommen', 'weltweit', 'Anerkennung', 'Basis', 'neu', 'vertrauen', '\\n ', 'Europa', 'aufbauen', 'wirtschaftlich', 'politisch', 'Spaltung', 'EU', 'verhindern']\n",
      "['Europa', 'bereit', 'Herausforderung', '\\n ', 'bewältigen', 'Folge', 'Coronapandemie', 'Klimawandel', 'Terrorismus', 'Migration', 'freie', '\\n ', 'Demokrat', 'außenpolitisch', 'stark', 'EU', '\\n ', 'Wert', 'Interesse', 'Souveränität', 'schützen', '\\n ', 'autokratisch', 'Machtstreben', 'entgenstellen', 'mutig', 'Reform', 'Aufgabe', 'Arbeitsweise', 'Institution', '\\n ', 'EU', 'innen', 'demokratisch', 'wirtschaftlich', 'stark', '\\n ', 'außen', 'handlungsfähig', 'EU', '\\n ', 'echt', 'Global', 'Player', '\\n ', 'Zukunftskonferenz', 'neu', 'Schwung', 'Europa', '\\n ', 'nutzen', '\\n ', 'frei', 'Demokrat', 'unterstützen', 'Konferenz', 'Zukunft', '\\n ', 'Europa', 'politisch', 'Gremium', 'einjährig', 'Prozess', 'europäisch', 'Institution', 'zusammenkommen', 'Priorität', 'EU', 'Bürgerin', '\\n ', 'Bürger', 'Mitgliedstaat', 'diskutieren', '\\n ', 'Konferenz', 'zentral', 'Politikfeld', 'konzentrieren', '\\n ', 'gemeinsam', 'Zukunft', 'langfristig', 'Relevanz', '\\n ', 'Leitlinie', 'Ziel', 'Priorität', 'definieren', '\\n ', 'EU', 'Bewältigung', '\\n ', 'Pandemi', 'robust', 'aufstellen', 'Europa', 'Chancenkontinent', 'entwickeln', 'Fortschrittsmotor', 'mobilisieren', 'EU', 'institutionell', 'reformieren', '\\n ', 'bürgernäh', 'handlungsfähig', '\\n ', 'schließen', 'Vertragsänderung', 'anschließend', 'Mitgliedstaat', 'EU-Institution', '\\n ', 'angemessen', 'umsetzen', '\\n ', 'gemeinsam', 'Verfassung', 'europäisch', '\\n ', 'Union', 'Bundesstaat', '\\n ', 'frei', 'Demokrat', 'Abschluss', 'Konferenz', '\\n ', 'Zukunft', 'Europa', 'Verfassungskonvent', 'einberufen', '\\n ', 'Konvent', 'Dezentral', 'Föderal', 'verfasst', '\\n ', 'Union', 'rechtsverbindlich', 'Verfassung', 'Grundrechtekatalog', 'stark', 'Institution', 'geben', 'neu', '\\n ', 'europäisch', 'Verfassung', 'Bürgerin', 'Bürger', '\\n ', 'EU', 'gemeinsam', 'europäisch', 'Volksabstimmung', '\\n ', 'entscheiden', 'Grundlage', 'Föderal', '\\n ', 'dezentral', 'verfasst', 'europäisch', 'Bundesstaat', 'schaffen', '\\n ', 'Weg', 'erklärt', 'Gegenmodell', 'Rückfall', '\\n ', 'Europa', 'nationalstaatlich', 'Kleinstaaterei', 'einerseits', '\\n ', 'Schaffung', 'zentralisiert', 'europäisch', 'Superstaat', '\\n ', 'andererseits', 'möchten', 'europäischeIntegration', 'parallel', 'Europa', 'verschieden', 'Geschwindigkeit', 'vertiefen', '\\n ', 'stark', 'EU-Parlament', '\\n ', 'Transparenz', '\\n ', 'frei', 'Demokrat', 'fordern', 'institutionell', 'Reform', '\\n ', 'Transparenz', 'Effizienz', 'EU', 'europäisch', 'Parlament', 'einheitlich', 'Wahlrecht', '\\n ', 'staatenübergreifend', 'Liste', 'Spitzenkandidatinn', '\\n ', 'Spitzenkandidat', 'wählen', 'Vollparlament', 'Initiativrecht', 'aufwerten', 'europäisch', 'Parlament', 'fest', 'Tagungsort', '\\n ', 'Sitz', 'entscheiden', 'Kommissionspräsidentin', '\\n ', '-präsident', 'Spitzenkandidatin', 'Spitzenkandidat', 'EU-Parlament', 'Mehrheit', '\\n ', 'Stimme', 'vereinen', 'Parlament', '\\n ', 'Mehrheit', 'Mitglied', 'Misstrauen', 'aussprechen', 'anderer', 'Person', 'Kommissionspräsidente', '\\n ', 'wählen', 'Vorschlagsrecht', 'übrig', 'Kommissarinn', '\\n ', 'Kommissar', 'liegen', 'Kommissionspräsidentin', 'beziehungsweise', '-präsidenten', 'Parlament', '\\n ', 'Vorschlag', 'einzeln', 'bestätigen', 'EU-Kommission', '\\n ', 'höchstens', 'Kommissarinn', 'Kommissar', 'verkleinern', '\\n ', 'hierbei', 'klar', 'einfach', 'zurechenbar', '\\n ', 'Ressort', 'vergeben', 'EU-Zuständigkeit', 'entsprechen', 'Rat', 'europäisch', 'Union', 'Untergruppierung', 'öffentlich', '\\n ', 'strategisch', 'Souveränität', 'EU', 'anstreben', '\\n ', 'frei', 'Demokrat', 'unterstützen', 'Ziel', 'europäisch', '\\n ', 'Union', 'strategisch', 'Souveränität', 'erreichen', '\\n ', 'bedeuten', 'Linie', 'eigenständig', 'Handlungsfähigkeit', 'erforderlich', 'ausstatten', '\\n ', 'EU', 'Zukunft', 'Interesse', 'Wert', '\\n ', 'durchsetzen', 'wichtig', 'Bereich', '\\n ', 'Energieversorgung', 'Rohstoffimport', 'digital', 'Technologie', '\\n ', 'abhängig', 'verwundbar', 'Handelsund', 'Entwicklungspolitik', 'EU', 'Stärke', '\\n ', 'strategisch', 'einsetzen', 'gemeinsam', 'Sicherheitsund', 'Verteidigungspolitik', 'GSVP', 'militärisch', 'Fähigkeit', 'entwickeln', 'stehen', 'Widerspruch', 'transatlantisch', 'Partnerschaft', 'NATO', 'erhöhen', '\\n ', 'Gewicht', 'EU', 'Partnerin', 'Augenhöhe', 'Beitrag', '\\n ', 'liberal', 'Weltordnung', 'leisten', 'Wunsch', '\\n ', 'strategisch', 'Souveränität', 'weder', 'Protektionismus', '\\n ', 'Selbstisolation', 'führen', '\\n ', 'echt', 'gemeinsam', 'Außen', 'Sicherheitspolitik', '\\n ', 'Europa', '\\n ', 'frei', 'Demokrat', 'Gemeinsame', 'Außen', '\\n ', 'Sicherheitspolitik', 'GASP', 'EU', 'Name', '\\n ', 'verdienen', 'europäisch', 'Union', 'international', 'schnell', 'handlungsfähig', 'außen', 'Stimme', '\\n ', 'sprechen', 'fordern', 'Einstimmigkeit']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"de_core_news_sm\")\n",
    "    \n",
    "    def tokenize_documents(self, documents):\n",
    "        tokenized_docs = []\n",
    "        for doc in documents:\n",
    "            doc_tokens = self.nlp(doc)\n",
    "            tokenized_tokens = [token.text for token in doc_tokens]\n",
    "            tokenized_docs.append(tokenized_tokens)\n",
    "        return tokenized_docs\n",
    "    \n",
    "    def lemmatize_documents(self, tokenized_docs):\n",
    "        lemmatized_docs = []\n",
    "        for doc_tokens in tokenized_docs:\n",
    "            lemmatized_tokens = [token.lemma_ for token in self.nlp(' '.join(doc_tokens))]\n",
    "            lemmatized_docs.append(lemmatized_tokens)\n",
    "        return lemmatized_docs\n",
    "    \n",
    "    def remove_stopwords_punctuations_numbers(self, lemmatized_docs):\n",
    "        clean_docs = []\n",
    "        for lemmatized_tokens in lemmatized_docs:\n",
    "            lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not self.nlp.vocab[token].is_stop]\n",
    "            lemmatized_tokens_no_punct = [token for token in lemmatized_tokens_no_stopwords if not self.nlp.vocab[token].is_punct]\n",
    "            lemmatized_tokens_no_nums = [token for token in lemmatized_tokens_no_punct if not self.nlp.vocab[token].like_num]\n",
    "            clean_docs.append(lemmatized_tokens_no_nums)\n",
    "        return clean_docs\n",
    "\n",
    "documents = [\n",
    "    \"\"\"Unser Unions-Versprechen: Wir arbeiten für ein modernes Europa, das weltpolitikfähig ist, um\n",
    "die globalen Herausforderungen gemeinsam zu meistern. Dafür muss Europa handlungsfähiger,\n",
    "mutiger und entschlossener werden. Denn nur wenn es Europa gut geht, geht es auch Deutschland gut.\n",
    "Europa wird herausgefordert – von innen und von außen. Innerhalb Europas setzen Populisten\n",
    "von links und rechts die europäische Demokratie unter Druck. Zusätzlich erschweren Nationalismus und Eigeninteressen einiger EU-Mitgliedsstaaten immer wieder gemeinsame europäische\n",
    "Lösungen oder verhindern ein Auftreten der EU mit einer Stimme. Und schließlich ist die EU in\n",
    "zentralen Bereichen, wie etwa der Verteidigungspolitik, nicht so handlungsfähig, wie wir uns das\n",
    "wünschen.\n",
    "Auch von außen sehen wir den europäisch-abendländischen Leitgedanken der Demokratie wie\n",
    "auch der Sozialen Marktwirtschaft unter Druck und im Wettbewerb mit konkurrierenden Gesellschafts- und Wirtschaftsmodellen. Freier Welthandel mit offenen Märkten, der uns Wohlstand gebracht hat, ist keine Selbstverständlichkeit mehr.\n",
    "Unsere Antwort auf diese Herausforderung lautet: Mehr Europa! Denn nur gemeinsam mit unseren europäischen Partnern werden wir die Herausforderungen meistern. Dafür brauchen wir\n",
    "schnellere und dynamischere Entscheidungen dort, wo es europäische Lösungen und entschlossenes Handeln auf internationaler Ebene braucht. Dabei gilt die Formel: Nicht jedes Problem in\n",
    "Europa ist ein Problem für Europa. Gleichzeitig werden wir das Modernisierungsjahrzehnt auch\n",
    "auf Europa erstrecken: Wir investieren in Technologien und Innovationen, damit Europas Wirtschaft auch in Zukunft Garant für Wohlstand, Arbeitsplätze und Nachhaltigkeit bleibt. Wir investieren in Europas Sicherheit, ob nach innen oder außen, damit auch unsere Kinder und Enkel\n",
    "in Europa in Frieden, Freiheit und Sicherheit leben können\"\"\",\n",
    "\n",
    "    \"\"\"Die AfD steht für die Freiheit und Selbstbestimmung der\n",
    "europäischen Nationen. Wir bekennen uns zu einem\n",
    "Europa der Vaterländer als einer Gemeinschaft\n",
    "souveräner Staaten, die auf all jenen Gebieten\n",
    "zusammenarbeiten, die gemeinsam besser gestaltet\n",
    "werden können. Dazu gehört insbesondere ein freier\n",
    "Handel mit fairem Wettbewerb.\n",
    "Eine staatsähnliche Europäische Union, wie sie von den\n",
    "etablierten Parteien angestrebt wird, halten wir im Sinne\n",
    "eines prosperierenden und friedlichen Europas für\n",
    "kontraproduktiv. Selbstverantwortliche und von\n",
    "lebendigen Demokratien gestaltete Nationalstaaten sind\n",
    "durch übernationale Einrichtungen nicht ersetzbar.\n",
    "Eine Gruppe benachbarter Staaten kann sehr gut auf\n",
    "völkerrechtlicher Basis konstruktiv und friedlich\n",
    "kooperieren. Der Versuch jedoch, aus derzeit 27 oder\n",
    "noch mehr Staaten mit jeweils eigenen Sprachen,\n",
    "Kulturen und historischen Erfahrungen einen wie auch\n",
    "immer ausgestalteten Gesamtstaat zu bilden, muss\n",
    "scheitern. Ein solches Gebilde verfügt weder über ein\n",
    "Staatsvolk, noch über das erforderliche Mindestmaß an\n",
    "kultureller Identität, welche notwendige Voraussetzungen für gelingende Staaten sind. Wir wollen den\n",
    "souveränen, demokratischen Nationalstaat erhalten.\n",
    "Nur dort kann Volkssouveränität gelebt werden, die\n",
    "Mutter und das Herzstück der Demokratie.Eine Union europäischer Staaten wird nur dann eine\n",
    "Zukunft haben, wenn es gelingt, dem sich immer\n",
    "schneller drehenden Rad der Entdemokratisierung und\n",
    "Zentralisierung in die Speichen zu greifen, bevor die\n",
    "heutige EU durch die Pervertierung ihrer Gründungsidee an sich selbst zugrunde geht. Die wirtschaftsgeschichtlich ungewöhnliche Idee einer Einheitswährung für wirtschaftlich völlig unterschiedlich\n",
    "entwickelte Staaten ist gescheitert. Es handelte sich um\n",
    "eine politische Wunschvorstellung, die mit\n",
    "ökonomischen Gesetzen nicht in Einklang zu bringen ist.\n",
    "Mit der Einführung des sogenannten „CoronaWiederaufbaupaktes“ wurde die Transferunion in eine\n",
    "neue Dimension gehoben. Diese Transferunion steht\n",
    "nicht nur im Widerspruch zu den europäischen\n",
    "Verträgen und den Versprechen der deutschen Politiker,\n",
    "sondern wird den Abstieg aller europäischen\n",
    "Volkswirtschaften und Konflikte zwischen den Staaten\n",
    "zur Folge haben.\n",
    "Ein vergleichbares Versagen zeigt die EU in der jahrelang\n",
    "anhaltenden Migrationskrise. Auch das Krisenmanagement in der Corona-Krise war verheerend.\n",
    "Die Fehlentwicklungen in der Klima- und Energiepolitik\n",
    "treibt die EU durch unvorstellbar kostspielige Gesetzesund Subventionspakete voran. Das langfristige Unheil\n",
    "dieser Verschuldungs- und Umverteilungs-Eskapaden\n",
    "wird vor allem den deutschen Steuerzahler treffen. Die Vehemenz, mit welcher die Europäische Union die\n",
    "Transformation zum planwirtschaftlichen Superstaat\n",
    "in den letzten Jahren vorangetrieben hat, hat uns zu der\n",
    "Erkenntnis gebracht, dass sich unsere grundlegenden\n",
    "Reformansätze in dieser EU nicht verwirklichen lassen.\n",
    "Wir halten einen Austritt Deutschlands aus der\n",
    "Europäischen Union und die Gründung einer neuen\n",
    "europäischen Wirtschafts- und Interessengemeinschaft\n",
    "für notwendig\"\"\",\n",
    "\n",
    "\"\"\"Grenzenlos reisen, arbeiten, studieren oder leben. Die Europäische Union (EU) hat das Leben von\n",
    "Millionen von Menschen geprägt, neue Möglichkeiten und Freiheiten eröffnet und den unermesslichen Wert kultureller Vielfalt für unsere Gesellschaften für viele erlebbar gemacht. Sie hat gezeigt, dass wir gemeinsam mehr erreichen können. Darauf wird es in Zukunft ganz besonders\n",
    "ankommen. \n",
    "In einem von globalem Wettbewerb geprägten Umfeld können wir unsere europäischen Werte\n",
    "und Interessen nur behaupten, wenn Europa nach innen geeint und nach außen handlungsfähig\n",
    "ist. Wir wollen die Freiheit und Rechtstaatlichkeit in Europa schützen und die EU zur modernsten\n",
    "Demokratie der Welt machen. Wir wollen, dass Europa auch beim Klimaschutz Vorreiter wird. Wir\n",
    "wollen ein digital souveränes Europa auf der Basis einer wertebasierten digitalen Wirtschaft. Mit\n",
    "Investitionen in unsere gemeinsame Wirtschafts- und Innovationskraft stärken wir Europa als den\n",
    "modernsten, sozialsten, nachhaltigsten und wettbewerbsfähigsten Wirtschaftsraum der Welt und\n",
    "sichern so die Grundlagen unseres Wohlstands. Damit schaffen wir die Voraussetzungen für ein\n",
    "souveränes Europa, das für soziale Gerechtigkeit, Wohlstand und Menschenrechte steht und sich\n",
    "geschlossen für eine gerechtere, friedlichere und nachhaltigere Welt einsetzt. \n",
    "Nur miteinander werden wir eine humanitäre und solidarische Flüchtlingspolitik gewährleisten.\n",
    "Demokratie, Freiheit und Rechtsstaatlichkeit sind das Fundament unserer Gemeinschaft. Wir werden es nicht zulassen, dass nationalistischer Hass und populistische Hetze Europa spalten. \n",
    "Wir meinen es ernst mit der europäischen Solidarität. Wir haben zu Beginn der Pandemie schnell\n",
    "und kraftvoll gehandelt und den größten Wiederaufbaufonds der Geschichte der Europäischen\n",
    "Union auf den Weg gebracht – eine Solidarleistung, die die sozialen Folgen der Corona-Krise abmildert und die gleichzeitig den sozial-ökologischen Wandel vorantreibt und Innovationen fördert.\n",
    "Europa bekommt dafür weltweite Anerkennung. Auf dieser Basis wollen wir neues Vertrauen in\n",
    "Europa aufbauen und eine wirtschaftliche und politische Spaltung der EU verhindern. \"\"\",\n",
    "\"\"\"Europa muss bereit sein, die großen Herausforderungen\n",
    "unserer Zeit zu bewältigen – die Folgen der Coronapandemie, den Klimawandel, Terrorismus und Migration. Wir Freie\n",
    "Demokraten wollen eine außenpolitisch starke EU, die ihre\n",
    "Werte, ihre Interessen und Souveränität schützt sowie sich\n",
    "autokratischem Machtstreben entgegenstellt. Mit mutigen Reformen ihrer Aufgaben, Arbeitsweise und Institutionen wollen\n",
    "wir die EU nach innen demokratisch und wirtschaftlich stark\n",
    "sowie nach außen handlungsfähig machen. So wird die EU\n",
    "zu einem echten Global Player.\n",
    "Zukunftskonferenz für neuen Schwung in Europa\n",
    "nutzen\n",
    "Wir Freie Demokraten unterstützen die Konferenz zur Zukunft\n",
    "Europas. In diesem politischen Gremium sollen in einem einjährigen Prozess alle europäischen Institutionen zusammenkommen, um die Prioritäten für die EU mit Bürgerinnen und\n",
    "Bürgern aus allen Mitgliedstaaten zu diskutieren. Wir wollen,\n",
    "dass sich die Konferenz auf zentrale Politikfelder konzentriert,\n",
    "die für unsere gemeinsame Zukunft langfristig von Relevanz\n",
    "sind. Dafür müssen Leitlinien, Ziele und Prioritäten definiert\n",
    "werden. Für uns geht es darum, die EU bei der Bewältigung\n",
    "von Pandemien robuster aufzustellen, Europa als Chancenkontinent zu entwickeln und als Fortschrittsmotor zu mobilisieren. Außerdem wollen wir die EU institutionell reformieren,\n",
    "um sie bürgernäher und handlungsfähiger zu machen. Das\n",
    "schließt auch Vertragsänderungen ein. Diese müssen anschließend von den Mitgliedstaaten und von den EU-Institutionen\n",
    "angemessen umgesetzt werden.\n",
    "Für eine gemeinsame Verfassung der Europäischen\n",
    "Union als Bundesstaat\n",
    "Wir Freie Demokraten wollen nach Abschluss der Konferenz\n",
    "zur Zukunft Europas einen Verfassungskonvent einberufen.\n",
    "Dieser Konvent sollte einer dezentral und föderal verfassten\n",
    "Union eine rechtsverbindliche Verfassung mit einem Grundrechtekatalog und starken Institutionen geben. Über die neue\n",
    "Europäische Verfassung sollen die Bürgerinnen und Bürger\n",
    "der EU in einer gemeinsamen europäischen Volksabstimmung\n",
    "entscheiden und damit die Grundlage für einen föderal und\n",
    "dezentral verfassten Europäischen Bundesstaat schaffen.\n",
    "Dieser Weg ist das erklärte Gegenmodell zum Rückfall\n",
    "Europas in nationalstaatliche Kleinstaaterei einerseits oder\n",
    "die Schaffung eines zentralisierten europäischen Superstaats\n",
    "andererseits. Bis dahin möchten wir, dass die europäischeIntegration parallel durch ein „Europa der verschiedenen Geschwindigkeiten“ vertieft wird.\n",
    "Für ein starkes EU-Parlament und mehr\n",
    "Transparenz\n",
    "Wir Freie Demokraten fordern institutionelle Reformen für\n",
    "mehr Transparenz und Effizienz in der EU. Das Europäische Parlament soll nach einem einheitlichen Wahlrecht mit\n",
    "staatenübergreifenden Listen und Spitzenkandidatinnen und\n",
    "Spitzenkandidaten gewählt werden. Es muss zu einem Vollparlament mit Initiativrecht aufgewertet werden. Das Europäische Parlament soll einen festen Tagungsort haben und selbst\n",
    "über seinen Sitz entscheiden können. Kommissionspräsidentin\n",
    "oder -präsident wird die Spitzenkandidatin oder der Spitzenkandidat, die oder der im EU-Parlament eine Mehrheit der\n",
    "Stimmen auf sich vereint. Das Parlament kann ihr oder ihm\n",
    "durch die Mehrheit seiner Mitglieder das Misstrauen aussprechen und eine andere Person zum Kommissionspräsidenten\n",
    "wählen. Das Vorschlagsrecht für die übrigen Kommissarinnen\n",
    "und Kommissare liegt bei der Kommissionspräsidentin beziehungsweise beim -präsidenten und das Parlament muss die\n",
    "Vorschläge einzeln bestätigen. Die EU-Kommission sollte auf\n",
    "höchstens 18 Kommissarinnen und Kommissare verkleinert\n",
    "werden. Hierbei müssen klare und einfach zurechenbare\n",
    "Ressorts vergeben werden, die den EU-Zuständigkeiten entsprechen. Der Rat der Europäischen Union und seine Untergruppierungen sollen öffentlich tagen.\n",
    "Offene Strategische Souveränität der EU anstreben\n",
    "Wir Freie Demokraten unterstützen das Ziel der Europäischen\n",
    "Union, eine größere strategische Souveränität zu erreichen.\n",
    "Für uns bedeutet dies in erster Linie eigenständige Handlungsfähigkeit. Mit den erforderlichen Mitteln ausgestattet, könnte\n",
    "die EU in Zukunft ihre eigenen Interessen und Werte besser\n",
    "durchsetzen und in wichtigen Bereichen wie zum Beispiel\n",
    "Energieversorgung, Rohstoffimporte und digitale Technologie\n",
    "weniger abhängig und verwundbar werden. In der Handelsund Entwicklungspolitik muss die EU die eigenen Stärken\n",
    "strategischer einsetzen und in der Gemeinsamen Sicherheitsund Verteidigungspolitik (GSVP) eigene militärische Fähigkeiten entwickeln. Das steht nicht im Widerspruch zur transatlantischen Partnerschaft und zur NATO, sondern erhöht das\n",
    "Gewicht der EU, als Partnerin auf Augenhöhe einen Beitrag\n",
    "zur liberalen Weltordnung zu leisten. Der Wunsch nach\n",
    "strategischer Souveränität darf aber weder zu Protektionismus\n",
    "noch zu einer Selbstisolation führen.\n",
    "Echte Gemeinsame Außen- und Sicherheitspolitik in\n",
    "Europa\n",
    "Wir Freie Demokraten wollen eine Gemeinsame Außen- und\n",
    "Sicherheitspolitik (GASP) der EU, die den Namen auch\n",
    "verdient. Die Europäische Union muss international schneller handlungsfähig sein und nach außen mit einer Stimme\n",
    "sprechen. Wir fordern deshalb, dass die Einstimmigkeit im\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Tokenization\n",
    "tokenized = preprocessor.tokenize_documents(documents)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized = preprocessor.lemmatize_documents(tokenized)\n",
    "\n",
    "# Removing Stopwords, Punctuation, and Numbers\n",
    "cleaned = preprocessor.remove_stopwords_punctuations_numbers(lemmatized)\n",
    "\n",
    "# Print cleaned documents\n",
    "print(\"\\nCleaned Documents:\")\n",
    "for tokens in cleaned:\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Representation Scheme**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag-of-Words Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\\n</th>\n",
       "      <th>Antwort</th>\n",
       "      <th>Arbeitsplatz</th>\n",
       "      <th>Bereich</th>\n",
       "      <th>Demokratie</th>\n",
       "      <th>Deutschland</th>\n",
       "      <th>Druck</th>\n",
       "      <th>EU</th>\n",
       "      <th>EU-Mitgliedsstaat</th>\n",
       "      <th>Ebene</th>\n",
       "      <th>...</th>\n",
       "      <th>verschieden</th>\n",
       "      <th>vertiefen</th>\n",
       "      <th>verwundbar</th>\n",
       "      <th>wichtig</th>\n",
       "      <th>wählen</th>\n",
       "      <th>zentralisiert</th>\n",
       "      <th>zurechenbar</th>\n",
       "      <th>zusammenkommen</th>\n",
       "      <th>öffentlich</th>\n",
       "      <th>übrig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 489 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    \\n   Antwort  Arbeitsplatz  Bereich  Demokratie  Deutschland  Druck    EU  \\\n",
       "0  14.0      1.0           1.0      1.0         2.0          1.0    2.0   2.0   \n",
       "1  52.0      0.0           0.0      0.0         2.0          1.0    0.0   4.0   \n",
       "2  19.0      0.0           0.0      0.0         2.0          0.0    0.0   3.0   \n",
       "3  69.0      0.0           0.0      1.0         0.0          0.0    0.0  13.0   \n",
       "\n",
       "   EU-Mitgliedsstaat  Ebene  ...  verschieden  vertiefen  verwundbar  wichtig  \\\n",
       "0                1.0    1.0  ...          0.0        0.0         0.0      0.0   \n",
       "1                0.0    0.0  ...          0.0        0.0         0.0      0.0   \n",
       "2                0.0    0.0  ...          0.0        0.0         0.0      0.0   \n",
       "3                0.0    0.0  ...          1.0        1.0         1.0      1.0   \n",
       "\n",
       "   wählen  zentralisiert  zurechenbar  zusammenkommen  öffentlich  übrig  \n",
       "0     0.0            0.0          0.0             0.0         0.0    0.0  \n",
       "1     0.0            0.0          0.0             0.0         0.0    0.0  \n",
       "2     0.0            0.0          0.0             0.0         0.0    0.0  \n",
       "3     2.0            1.0          1.0             1.0         1.0    1.0  \n",
       "\n",
       "[4 rows x 489 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<489 unique tokens: ['\\n ', 'Antwort', 'Arbeitsplatz', 'Bereich', 'Demokratie']...>\n",
      "[[(0, 14), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 2), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 12), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 3), (20, 1), (21, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 2), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 2), (48, 1), (49, 1), (50, 2), (51, 1), (52, 1), (53, 4), (54, 1), (55, 1), (56, 1), (57, 3), (58, 1), (59, 1), (60, 1), (61, 2), (62, 1), (63, 2), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1)], [(0, 52), (4, 2), (5, 1), (7, 4), (13, 2), (15, 1), (38, 1), (39, 1), (42, 1), (48, 2), (53, 8), (55, 1), (57, 1), (60, 1), (69, 1), (76, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 2), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 7), (136, 1), (137, 1), (138, 1), (139, 1), (140, 2), (141, 1), (142, 1), (143, 1), (144, 4), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 2), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 2), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 2), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 2), (200, 2), (201, 1), (202, 1), (203, 1), (204, 2), (205, 1), (206, 1), (207, 2), (208, 1), (209, 2), (210, 1), (211, 1), (212, 1), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1)], [(0, 19), (4, 2), (7, 3), (13, 9), (15, 3), (18, 1), (20, 1), (38, 1), (39, 2), (41, 2), (42, 1), (43, 1), (45, 1), (48, 1), (53, 4), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (69, 1), (72, 2), (76, 1), (79, 2), (81, 1), (89, 2), (90, 1), (102, 1), (105, 1), (144, 2), (153, 1), (194, 1), (199, 2), (202, 1), (207, 2), (209, 1), (221, 1), (225, 1), (227, 1), (232, 1), (233, 1), (234, 1), (235, 1), (236, 1), (237, 1), (238, 1), (239, 1), (240, 1), (241, 1), (242, 1), (243, 1), (244, 1), (245, 1), (246, 1), (247, 1), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 1), (260, 1), (261, 1), (262, 3), (263, 2), (264, 1), (265, 1), (266, 1), (267, 1), (268, 1), (269, 1), (270, 1), (271, 2), (272, 1), (273, 1), (274, 1), (275, 1), (276, 1), (277, 1), (278, 1), (279, 1), (280, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (286, 1), (287, 1), (288, 1), (289, 1), (290, 1), (291, 1), (292, 1), (293, 1), (294, 1), (295, 1), (296, 1), (297, 1), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 1), (304, 1), (305, 1), (306, 1)], [(0, 69), (3, 1), (7, 13), (13, 8), (19, 1), (34, 2), (35, 1), (36, 1), (42, 4), (45, 2), (53, 11), (55, 5), (57, 5), (61, 3), (63, 1), (65, 1), (73, 1), (76, 1), (84, 1), (102, 1), (139, 1), (144, 5), (154, 1), (159, 1), (165, 1), (170, 1), (195, 1), (199, 2), (202, 1), (209, 1), (224, 1), (225, 1), (238, 1), (242, 2), (261, 1), (263, 2), (271, 1), (272, 1), (275, 1), (292, 1), (293, 1), (294, 1), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 1), (313, 2), (314, 1), (315, 1), (316, 2), (317, 2), (318, 2), (319, 1), (320, 1), (321, 6), (322, 1), (323, 1), (324, 1), (325, 2), (326, 1), (327, 1), (328, 1), (329, 1), (330, 1), (331, 1), (332, 1), (333, 2), (334, 1), (335, 1), (336, 1), (337, 1), (338, 1), (339, 1), (340, 1), (341, 1), (342, 1), (343, 1), (344, 1), (345, 1), (346, 3), (347, 1), (348, 1), (349, 2), (350, 2), (351, 1), (352, 2), (353, 3), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 2), (360, 1), (361, 1), (362, 1), (363, 2), (364, 1), (365, 1), (366, 1), (367, 4), (368, 1), (369, 1), (370, 1), (371, 1), (372, 1), (373, 2), (374, 1), (375, 1), (376, 1), (377, 2), (378, 1), (379, 1), (380, 1), (381, 1), (382, 1), (383, 1), (384, 1), (385, 2), (386, 1), (387, 1), (388, 4), (389, 2), (390, 1), (391, 1), (392, 1), (393, 1), (394, 1), (395, 2), (396, 1), (397, 3), (398, 1), (399, 1), (400, 1), (401, 1), (402, 1), (403, 1), (404, 1), (405, 1), (406, 1), (407, 2), (408, 1), (409, 1), (410, 1), (411, 1), (412, 1), (413, 1), (414, 1), (415, 1), (416, 1), (417, 1), (418, 1), (419, 1), (420, 1), (421, 1), (422, 1), (423, 1), (424, 1), (425, 1), (426, 1), (427, 1), (428, 1), (429, 1), (430, 2), (431, 1), (432, 1), (433, 1), (434, 1), (435, 1), (436, 1), (437, 1), (438, 1), (439, 2), (440, 1), (441, 2), (442, 1), (443, 1), (444, 1), (445, 1), (446, 2), (447, 1), (448, 1), (449, 1), (450, 1), (451, 1), (452, 2), (453, 1), (454, 1), (455, 1), (456, 1), (457, 1), (458, 1), (459, 1), (460, 1), (461, 1), (462, 1), (463, 1), (464, 1), (465, 1), (466, 1), (467, 1), (468, 1), (469, 4), (470, 4), (471, 1), (472, 1), (473, 2), (474, 1), (475, 1), (476, 2), (477, 1), (478, 1), (479, 1), (480, 1), (481, 1), (482, 1), (483, 2), (484, 1), (485, 1), (486, 1), (487, 1), (488, 1)]]\n"
     ]
    }
   ],
   "source": [
    "def create_bow_representation(preprocessed_docs):\n",
    "    dictionary = corpora.Dictionary(preprocessed_docs)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "    return dictionary, bow_corpus\n",
    "\n",
    "bow_dictionary, bow_corpus = create_bow_representation(cleaned)\n",
    "\n",
    "# Display bow representation\n",
    "print(\"\\nBag-of-Words Representation:\")\n",
    "num_terms = len(bow_dictionary)\n",
    "bow_matrix = [sparse2full(doc, num_terms) for doc in bow_corpus]\n",
    "bow_df = pd.DataFrame(bow_matrix, columns=[bow_dictionary[i] for i in range(num_terms)])\n",
    "\n",
    "display(bow_df)\n",
    "\n",
    "print(bow_dictionary)\n",
    "print(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abendländisch</th>\n",
       "      <th>abhängig</th>\n",
       "      <th>abmilderen</th>\n",
       "      <th>abschluss</th>\n",
       "      <th>abstieg</th>\n",
       "      <th>afd</th>\n",
       "      <th>all</th>\n",
       "      <th>anderer</th>\n",
       "      <th>andererseits</th>\n",
       "      <th>anerkennung</th>\n",
       "      <th>...</th>\n",
       "      <th>zurechenbar</th>\n",
       "      <th>zusammenarbeiten</th>\n",
       "      <th>zusammenkommen</th>\n",
       "      <th>zuständigkeit</th>\n",
       "      <th>zusätzlich</th>\n",
       "      <th>öffentlich</th>\n",
       "      <th>ökologisch</th>\n",
       "      <th>ökonomisch</th>\n",
       "      <th>übernational</th>\n",
       "      <th>übrig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.080242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08118</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.08118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040797</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 484 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abendländisch  abhängig  abmilderen  abschluss   abstieg       afd  \\\n",
       "0       0.080242  0.000000     0.00000   0.000000  0.000000  0.000000   \n",
       "1       0.000000  0.000000     0.00000   0.000000  0.063571  0.063571   \n",
       "2       0.000000  0.000000     0.08118   0.000000  0.000000  0.000000   \n",
       "3       0.000000  0.040797     0.00000   0.040797  0.000000  0.000000   \n",
       "\n",
       "        all   anderer  andererseits  anerkennung  ...  zurechenbar  \\\n",
       "0  0.000000  0.000000      0.000000      0.00000  ...     0.000000   \n",
       "1  0.063571  0.000000      0.000000      0.00000  ...     0.000000   \n",
       "2  0.000000  0.000000      0.000000      0.08118  ...     0.000000   \n",
       "3  0.000000  0.040797      0.040797      0.00000  ...     0.040797   \n",
       "\n",
       "   zusammenarbeiten  zusammenkommen  zuständigkeit  zusätzlich  öffentlich  \\\n",
       "0          0.000000        0.000000       0.000000    0.080242    0.000000   \n",
       "1          0.063571        0.000000       0.000000    0.000000    0.000000   \n",
       "2          0.000000        0.000000       0.000000    0.000000    0.000000   \n",
       "3          0.000000        0.040797       0.040797    0.000000    0.040797   \n",
       "\n",
       "   ökologisch  ökonomisch  übernational     übrig  \n",
       "0     0.00000    0.000000      0.000000  0.000000  \n",
       "1     0.00000    0.063571      0.063571  0.000000  \n",
       "2     0.08118    0.000000      0.000000  0.000000  \n",
       "3     0.00000    0.000000      0.000000  0.040797  \n",
       "\n",
       "[4 rows x 484 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_tfidf_representation(preprocessed_docs):\n",
    "    # Joining tokenized documents to form a list of strings\n",
    "    tokenized_texts = [' '.join(doc) for doc in preprocessed_docs]\n",
    "    \n",
    "    # Creating TF-IDF vectorizer and fitting on the tokenized documents\n",
    "    ifidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_representation = ifidf_vectorizer.fit_transform(tokenized_texts)\n",
    "    \n",
    "    feature_names = ifidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return tfidf_representation, feature_names\n",
    "\n",
    "tfidf_corpus, tfidf_dictionary = create_tfidf_representation(cleaned)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_corpus.toarray(), columns=tfidf_dictionary)\n",
    "\n",
    "# Display the TF-IDF DataFrame\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "display(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Doc2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc2Vec Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.499787</td>\n",
       "      <td>-0.143083</td>\n",
       "      <td>0.017328</td>\n",
       "      <td>0.221256</td>\n",
       "      <td>-0.079321</td>\n",
       "      <td>-0.334754</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>0.322655</td>\n",
       "      <td>0.110057</td>\n",
       "      <td>0.148774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115821</td>\n",
       "      <td>0.228295</td>\n",
       "      <td>0.037441</td>\n",
       "      <td>0.115284</td>\n",
       "      <td>0.279372</td>\n",
       "      <td>0.529804</td>\n",
       "      <td>0.112345</td>\n",
       "      <td>-0.404212</td>\n",
       "      <td>0.308842</td>\n",
       "      <td>-0.013352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.829516</td>\n",
       "      <td>-0.238605</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>0.317256</td>\n",
       "      <td>-0.098434</td>\n",
       "      <td>-0.545157</td>\n",
       "      <td>-0.017444</td>\n",
       "      <td>0.744780</td>\n",
       "      <td>0.160674</td>\n",
       "      <td>0.336821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.317950</td>\n",
       "      <td>0.417930</td>\n",
       "      <td>0.136088</td>\n",
       "      <td>0.237520</td>\n",
       "      <td>0.460966</td>\n",
       "      <td>0.855360</td>\n",
       "      <td>0.306440</td>\n",
       "      <td>-0.631492</td>\n",
       "      <td>0.549230</td>\n",
       "      <td>0.082380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.543838</td>\n",
       "      <td>-0.145329</td>\n",
       "      <td>0.094704</td>\n",
       "      <td>0.244404</td>\n",
       "      <td>-0.008970</td>\n",
       "      <td>-0.452424</td>\n",
       "      <td>0.052072</td>\n",
       "      <td>0.375649</td>\n",
       "      <td>0.080080</td>\n",
       "      <td>0.129864</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.176375</td>\n",
       "      <td>0.221774</td>\n",
       "      <td>-0.029042</td>\n",
       "      <td>0.076621</td>\n",
       "      <td>0.290467</td>\n",
       "      <td>0.591623</td>\n",
       "      <td>0.119414</td>\n",
       "      <td>-0.327665</td>\n",
       "      <td>0.417363</td>\n",
       "      <td>-0.023012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.029107</td>\n",
       "      <td>0.080963</td>\n",
       "      <td>0.314284</td>\n",
       "      <td>0.714926</td>\n",
       "      <td>-0.059304</td>\n",
       "      <td>-0.926330</td>\n",
       "      <td>0.095871</td>\n",
       "      <td>1.023324</td>\n",
       "      <td>0.299102</td>\n",
       "      <td>0.188790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205229</td>\n",
       "      <td>0.369072</td>\n",
       "      <td>-0.255363</td>\n",
       "      <td>0.304968</td>\n",
       "      <td>0.419156</td>\n",
       "      <td>1.129592</td>\n",
       "      <td>-0.117008</td>\n",
       "      <td>-0.411436</td>\n",
       "      <td>0.692227</td>\n",
       "      <td>-0.008827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.499787 -0.143083  0.017328  0.221256 -0.079321 -0.334754 -0.015171   \n",
       "1 -0.829516 -0.238605  0.061444  0.317256 -0.098434 -0.545157 -0.017444   \n",
       "2 -0.543838 -0.145329  0.094704  0.244404 -0.008970 -0.452424  0.052072   \n",
       "3 -1.029107  0.080963  0.314284  0.714926 -0.059304 -0.926330  0.095871   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0  0.322655  0.110057  0.148774  ... -0.115821  0.228295  0.037441  0.115284   \n",
       "1  0.744780  0.160674  0.336821  ... -0.317950  0.417930  0.136088  0.237520   \n",
       "2  0.375649  0.080080  0.129864  ... -0.176375  0.221774 -0.029042  0.076621   \n",
       "3  1.023324  0.299102  0.188790  ... -0.205229  0.369072 -0.255363  0.304968   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.279372  0.529804  0.112345 -0.404212  0.308842 -0.013352  \n",
       "1  0.460966  0.855360  0.306440 -0.631492  0.549230  0.082380  \n",
       "2  0.290467  0.591623  0.119414 -0.327665  0.417363 -0.023012  \n",
       "3  0.419156  1.129592 -0.117008 -0.411436  0.692227 -0.008827  \n",
       "\n",
       "[4 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_doc2vec_representation(preprocessed_docs):\n",
    "    tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(preprocessed_docs)]\n",
    "    \n",
    "    # hyperparameters should be optimized\n",
    "    max_epochs = 100\n",
    "    vec_size = 100\n",
    "    alpha = 0.025\n",
    "    \n",
    "    # Train Doc2Vec model\n",
    "    model = Doc2Vec(vector_size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1)\n",
    "    model.build_vocab(tagged_data)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        model.alpha -= 0.0002\n",
    "        model.min_alpha = model.alpha\n",
    "    \n",
    "    doc2vec_representation = [model.infer_vector(doc.words) for doc in tagged_data]\n",
    "    \n",
    "    return doc2vec_representation\n",
    "\n",
    "doc2vec_corpus = create_doc2vec_representation(cleaned)\n",
    "\n",
    "# Convert Doc2Vec corpus to a DataFrame\n",
    "doc2vec_df = pd.DataFrame(doc2vec_corpus)\n",
    "\n",
    "# Display the Doc2Vec DataFrame\n",
    "print(\"\\nDoc2Vec Representation:\")\n",
    "display(doc2vec_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Similarity Measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cosine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity (BoW Representation):\n",
      "MatrixSimilarity<4 docs, 489 features>\n",
      "Document 1: [1.0000011  0.65514827 0.77884656 0.70252967]\n",
      "Document 2: [0.65514827 1.000002   0.79203165 0.91583484]\n",
      "Document 3: [0.77884656 0.79203165 1.0000012  0.80644023]\n",
      "Document 4: [0.70252967 0.91583484 0.80644023 0.9999997 ]\n",
      "\n",
      "Cosine Similarity (TF-IDF Representation):\n",
      "Document 1: [1.         0.14379777 0.37228733 0.25764228]\n",
      "Document 2: [0.14379777 1.         0.20173517 0.17857448]\n",
      "Document 3: [0.37228733 0.20173517 1.         0.23416249]\n",
      "Document 4: [0.25764228 0.17857448 0.23416249 1.        ]\n",
      "\n",
      "Cosine Similarity (Doc2Vec Representation):\n",
      "Document 1: [0.9999998  0.96773213 0.9787329  0.8878911 ]\n",
      "Document 2: [0.96773213 0.9999999  0.95247895 0.8509658 ]\n",
      "Document 3: [0.9787329  0.95247895 0.99999994 0.9160059 ]\n",
      "Document 4: [0.8878911 0.8509658 0.9160059 0.9999998]\n"
     ]
    }
   ],
   "source": [
    "def calculate_cosine_similarity(corpus, similarity_type='bow'):\n",
    "    if similarity_type == 'bow':\n",
    "        index = similarities.MatrixSimilarity(corpus) #calculates automaticly cosine similarity\n",
    "    elif similarity_type == 'tfidf' or similarity_type == 'doc2vec':\n",
    "        index = cosine_similarity(corpus)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid similarity_type. Choose 'bow', 'doc2vec' or 'tfidf'.\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "bow_similarity_index = calculate_cosine_similarity(bow_corpus, similarity_type='bow')\n",
    "print(\"\\nCosine Similarity (BoW Representation):\")\n",
    "print(bow_similarity_index)\n",
    "for i, sims in enumerate(bow_similarity_index):\n",
    "    print(f\"Document {i + 1}: {sims}\")\n",
    "\n",
    "\n",
    "tfidf_similarity_index = calculate_cosine_similarity(tfidf_corpus, similarity_type='tfidf')\n",
    "print(\"\\nCosine Similarity (TF-IDF Representation):\")\n",
    "for i, sims in enumerate(tfidf_similarity_index):\n",
    "    print(f\"Document {i + 1}: {sims}\")\n",
    "\n",
    "\n",
    "doc2vec_similarity_index = calculate_cosine_similarity(doc2vec_corpus, similarity_type='doc2vec')\n",
    "print(\"\\nCosine Similarity (Doc2Vec Representation):\")\n",
    "for i, sims in enumerate(doc2vec_similarity_index):\n",
    "    print(f\"Document {i + 1}: {sims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Eucliadian for bow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Similarity Matrix (bow representation):\n",
      "[[1.         0.0222729  0.05882353 0.01601094]\n",
      " [0.0222729  1.         0.02532687 0.02934645]\n",
      " [0.05882353 0.02532687 1.         0.01729377]\n",
      " [0.01601094 0.02934645 0.01729377 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_euclidean_similarity(corpus):\n",
    "    num_terms = max(token_id for doc in corpus for token_id, _ in doc) + 1\n",
    "    matrix = [sparse2full(doc, num_terms) for doc in corpus]\n",
    "    \n",
    "    # Calculate Euclidean similarity\n",
    "    num_docs = len(matrix)\n",
    "    similarity_matrix = np.zeros((num_docs, num_docs))\n",
    "    for i, doc1 in enumerate(matrix):\n",
    "        for j, doc2 in enumerate(matrix):\n",
    "            similarity_matrix[i, j] = 1 / (1 + euclidean(doc1, doc2))\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Calculate Euclidean similarity bow\n",
    "euclidean_similarity_bow = calculate_euclidean_similarity(bow_corpus)\n",
    "print(\"Euclidean Similarity Matrix (bow representation):\")\n",
    "print(euclidean_similarity_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Euclidean Distances (TF-IDF Representation):\n",
      "Document 1: [0.         1.30858873 1.12045765 1.218489  ]\n",
      "Document 2: [1.30858873 0.         1.26353855 1.28173751]\n",
      "Document 3: [1.12045765 1.26353855 0.         1.23760859]\n",
      "Document 4: [1.218489   1.28173751 1.23760859 0.        ]\n",
      "\n",
      "Euclidean Distances (Doc2Vec Representation):\n",
      "Document 1: [0.        1.8019334 0.5453893 3.273485 ]\n",
      "Document 2: [1.8019334 0.        1.6538728 2.675023 ]\n",
      "Document 3: [0.5453893 1.6538728 0.        2.966308 ]\n",
      "Document 4: [3.273485 2.675023 2.966308 0.      ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def calculate_distance(corpus, distance_type='bow'):\n",
    "    if distance_type == 'tfidf' or distance_type == 'doc2vec':\n",
    "        distance = euclidean_distances(corpus)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid distance_type. Choose 'bow', 'doc2vec', or 'tfidf'.\")\n",
    "    \n",
    "    return distance\n",
    "\n",
    "\n",
    "# Calculate Euclidean distances for TF-IDF representation\n",
    "tfidf_distance_index = calculate_distance(tfidf_corpus, distance_type='tfidf')\n",
    "print(\"\\nEuclidean Distances (TF-IDF Representation):\")\n",
    "for i, dists in enumerate(tfidf_distance_index):\n",
    "    print(f\"Document {i + 1}: {dists}\")\n",
    "\n",
    "# Assuming 'doc2vec_corpus' contains your Doc2Vec representations\n",
    "# Calculate Euclidean distances for Doc2Vec representation\n",
    "doc2vec_distance_index = calculate_distance(doc2vec_corpus, distance_type='doc2vec')\n",
    "print(\"\\nEuclidean Distances (Doc2Vec Representation):\")\n",
    "for i, dists in enumerate(doc2vec_distance_index):\n",
    "    print(f\"Document {i + 1}: {dists}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
