{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m spacy download de_core_news_sm\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, similarities\n",
    "from gensim.matutils import sparse2full\n",
    "from gensim.matutils import sparse2full\n",
    "from scipy.spatial.distance import euclidean\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Information**\n",
    "\n",
    "Prefer spacy instead of nltk because it's faster and we want to compute large texts.\n",
    "Lemmatization and Stemming are redundand and different aproaches. They normaly are not used together. Decided for Lemmatizing because its integrated into spacy module:\n",
    "\n",
    "\n",
    "Sources:\n",
    "- Text Similarity Measures in News Articles by Vector Space Model Using NLP (https://link.springer.com/article/10.1007/s40031-020-00501-5)\n",
    "- Compare documents similarity using Python | NLP (https://dev.to/thedevtimeline/compare-documents-similarity-using-python-nlp-4odp)\n",
    "- What is gensim.similarities.MatrixSimilarity() function? (https://www.educative.io/answers/what-is-gensimsimilaritiesmatrixsimilarity-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Das ist ein Text.\",\n",
    "    \"Ich habe einen Text geschrieben.\",\n",
    "    \"Ich habe mehrere Texte geschrieben!\",\n",
    "    \"Das sind viele texte. Insgesamt sind es 4.\"\n",
    "]\n",
    "\n",
    "sp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Documents:\n",
      "['Das', 'ist', 'ein', 'Text', '.']\n",
      "['Ich', 'habe', 'einen', 'Text', 'geschrieben', '.']\n",
      "['Ich', 'habe', 'mehrere', 'Texte', 'geschrieben', '!']\n",
      "['Das', 'sind', 'viele', 'texte', '.', 'Insgesamt', 'sind', 'es', '4.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    doc_tokens = sp(doc)\n",
    "    tokenized_tokens = [token.text for token in doc_tokens]\n",
    "    tokenized_docs.append(tokenized_tokens)\n",
    "\n",
    "print(\"Tokenized Documents:\")\n",
    "for doc_tokens in tokenized_docs:\n",
    "    print(doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Lammatize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents:\n",
      "['der', 'sein', 'ein', 'Text', '--']\n",
      "['ich', 'haben', 'ein', 'Text', 'schreiben', '--']\n",
      "['ich', 'haben', 'mehrere', 'Text', 'schreiben', '--']\n",
      "['der', 'sein', 'vieler', 'Text', '--', 'insgesamt', 'sein', 'es', '4.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs = []\n",
    "for doc_tokens in tokenized_docs:\n",
    "    lemmatized_tokens = [token.lemma_ for token in sp(' '.join(doc_tokens))]\n",
    "    lemmatized_docs.append(lemmatized_tokens)\n",
    "\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Remove Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents without Stop Words:\n",
      "['Text', '--']\n",
      "['Text', 'schreiben', '--']\n",
      "['mehrere', 'Text', 'schreiben', '--']\n",
      "['vieler', 'Text', '--', 'insgesamt', '4.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs_no_stopwords = []\n",
    "\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not sp.vocab[token].is_stop]\n",
    "    lemmatized_docs_no_stopwords.append(lemmatized_tokens_no_stopwords)\n",
    "\n",
    "print(\"\\nLemmatized Documents without Stop Words:\")\n",
    "for lemmatized_tokens in lemmatized_docs_no_stopwords:\n",
    "    print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4.Eleminate Puctation Marks & Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents without Stop Words, Punctuation, and Numbers:\n",
      "['Text']\n",
      "['Text', 'schreiben']\n",
      "['mehrere', 'Text', 'schreiben']\n",
      "['vieler', 'Text', 'insgesamt']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs_no_stopwords_punct_nums = []\n",
    "\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not sp.vocab[token].is_stop]\n",
    "    \n",
    "    # Remove punctuation tokens\n",
    "    lemmatized_tokens_no_punct = [token for token in lemmatized_tokens_no_stopwords if not sp.vocab[token].is_punct]\n",
    "    \n",
    "    # Remove number tokens\n",
    "    lemmatized_tokens_no_nums = [token for token in lemmatized_tokens_no_punct if not sp.vocab[token].like_num]\n",
    "    \n",
    "    lemmatized_docs_no_stopwords_punct_nums.append(lemmatized_tokens_no_nums)\n",
    "\n",
    "print(\"\\nLemmatized Documents without Stop Words, Punctuation, and Numbers:\")\n",
    "for tokens in lemmatized_docs_no_stopwords_punct_nums:\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Documents:\n",
      "['spannend', 'Text']\n",
      "['Text', 'schreiben']\n",
      "['mehrere', 'Text', 'schreiben']\n",
      "['vieler', 'Text', 'insgesamt']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"de_core_news_sm\")\n",
    "    \n",
    "    def tokenize_documents(self, documents):\n",
    "        tokenized_docs = []\n",
    "        for doc in documents:\n",
    "            doc_tokens = self.nlp(doc)\n",
    "            tokenized_tokens = [token.text for token in doc_tokens]\n",
    "            tokenized_docs.append(tokenized_tokens)\n",
    "        return tokenized_docs\n",
    "    \n",
    "    def lemmatize_documents(self, tokenized_docs):\n",
    "        lemmatized_docs = []\n",
    "        for doc_tokens in tokenized_docs:\n",
    "            lemmatized_tokens = [token.lemma_ for token in self.nlp(' '.join(doc_tokens))]\n",
    "            lemmatized_docs.append(lemmatized_tokens)\n",
    "        return lemmatized_docs\n",
    "    \n",
    "    def remove_stopwords_punctuations_numbers(self, lemmatized_docs):\n",
    "        clean_docs = []\n",
    "        for lemmatized_tokens in lemmatized_docs:\n",
    "            lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not self.nlp.vocab[token].is_stop]\n",
    "            lemmatized_tokens_no_punct = [token for token in lemmatized_tokens_no_stopwords if not self.nlp.vocab[token].is_punct]\n",
    "            lemmatized_tokens_no_nums = [token for token in lemmatized_tokens_no_punct if not self.nlp.vocab[token].like_num]\n",
    "            clean_docs.append(lemmatized_tokens_no_nums)\n",
    "        return clean_docs\n",
    "\n",
    "documents = [\n",
    "    \"Das ist ein spannender Text.\",\n",
    "    \"Ich habe einen Text geschrieben.\",\n",
    "    \"Ich habe mehrere Texte geschrieben!\",\n",
    "    \"Das sind viele texte. Insgesamt sind es 4.\"\n",
    "]\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Tokenization\n",
    "tokenized = preprocessor.tokenize_documents(documents)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized = preprocessor.lemmatize_documents(tokenized)\n",
    "\n",
    "# Removing Stopwords, Punctuation, and Numbers\n",
    "cleaned = preprocessor.remove_stopwords_punctuations_numbers(lemmatized)\n",
    "\n",
    "# Print cleaned documents\n",
    "print(\"\\nCleaned Documents:\")\n",
    "for tokens in cleaned:\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Representation Scheme**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bag-of-Words Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>spannend</th>\n",
       "      <th>schreiben</th>\n",
       "      <th>mehrere</th>\n",
       "      <th>insgesamt</th>\n",
       "      <th>vieler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Text  spannend  schreiben  mehrere  insgesamt  vieler\n",
       "0   1.0       1.0        0.0      0.0        0.0     0.0\n",
       "1   1.0       0.0        1.0      0.0        0.0     0.0\n",
       "2   1.0       0.0        1.0      1.0        0.0     0.0\n",
       "3   1.0       0.0        0.0      0.0        1.0     1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<6 unique tokens: ['Text', 'spannend', 'schreiben', 'mehrere', 'insgesamt']...>\n",
      "[[(0, 1), (1, 1)], [(0, 1), (2, 1)], [(0, 1), (2, 1), (3, 1)], [(0, 1), (4, 1), (5, 1)]]\n"
     ]
    }
   ],
   "source": [
    "def create_bow_representation(preprocessed_docs):\n",
    "    dictionary = corpora.Dictionary(preprocessed_docs)\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "    return dictionary, bow_corpus\n",
    "\n",
    "bow_dictionary, bow_corpus = create_bow_representation(cleaned)\n",
    "\n",
    "# Display bow representation\n",
    "print(\"\\nBag-of-Words Representation:\")\n",
    "num_terms = len(bow_dictionary)\n",
    "bow_matrix = [sparse2full(doc, num_terms) for doc in bow_corpus]\n",
    "bow_df = pd.DataFrame(bow_matrix, columns=[bow_dictionary[i] for i in range(num_terms)])\n",
    "\n",
    "display(bow_df)\n",
    "\n",
    "print(bow_dictionary)\n",
    "print(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>insgesamt</th>\n",
       "      <th>mehrere</th>\n",
       "      <th>schreiben</th>\n",
       "      <th>spannend</th>\n",
       "      <th>text</th>\n",
       "      <th>vieler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.886548</td>\n",
       "      <td>0.462637</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.551939</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.726641</td>\n",
       "      <td>0.572892</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379192</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346182</td>\n",
       "      <td>0.663385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   insgesamt   mehrere  schreiben  spannend      text    vieler\n",
       "0   0.000000  0.000000   0.000000  0.886548  0.462637  0.000000\n",
       "1   0.000000  0.000000   0.833884  0.000000  0.551939  0.000000\n",
       "2   0.000000  0.726641   0.572892  0.000000  0.379192  0.000000\n",
       "3   0.663385  0.000000   0.000000  0.000000  0.346182  0.663385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_tfidf_representation(preprocessed_docs):\n",
    "    # Joining tokenized documents to form a list of strings\n",
    "    tokenized_texts = [' '.join(doc) for doc in preprocessed_docs]\n",
    "    \n",
    "    # Creating TF-IDF vectorizer and fitting on the tokenized documents\n",
    "    ifidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_representation = ifidf_vectorizer.fit_transform(tokenized_texts)\n",
    "    \n",
    "    feature_names = ifidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return tfidf_representation, feature_names\n",
    "\n",
    "tfidf_corpus, tfidf_dictionary = create_tfidf_representation(cleaned)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_corpus.toarray(), columns=tfidf_dictionary)\n",
    "\n",
    "# Display the TF-IDF DataFrame\n",
    "print(\"\\nTF-IDF Representation:\")\n",
    "display(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Similarity Measures**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cosine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity (BoW Representation):\n",
      "MatrixSimilarity<4 docs, 6 features>\n",
      "Document 1: [0.99999994 0.49999997 0.40824828 0.40824828]\n",
      "Document 2: [0.49999997 0.99999994 0.81649655 0.40824828]\n",
      "Document 3: [0.40824828 0.81649655 0.99999994 0.3333333 ]\n",
      "Document 4: [0.40824828 0.40824828 0.3333333  0.99999994]\n",
      "\n",
      "Cosine Similarity (TF-IDF Representation):\n",
      "Document 1: [1.         0.2553478  0.17542823 0.16015653]\n",
      "Document 2: [0.2553478  1.         0.68701684 0.19107127]\n",
      "Document 3: [0.17542823 0.68701684 0.9999999  0.13126917]\n",
      "Document 4: [0.16015653 0.19107127 0.13126917 1.        ]\n"
     ]
    }
   ],
   "source": [
    "def calculate_cosine_similarity(corpus, similarity_type='bow'):\n",
    "    if similarity_type == 'bow':\n",
    "        index = similarities.MatrixSimilarity(corpus) #calculates automaticly cosine similarity\n",
    "    elif similarity_type == 'tfidf':\n",
    "        index = similarities.MatrixSimilarity(corpus, num_features=len(corpus))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid similarity_type. Choose 'bow' or 'tfidf'.\")\n",
    "    \n",
    "    return index\n",
    "\n",
    "bow_similarity_index = calculate_cosine_similarity(bow_corpus, similarity_type='bow')\n",
    "print(\"\\nCosine Similarity (BoW Representation):\")\n",
    "print(bow_similarity_index)\n",
    "for i, sims in enumerate(bow_similarity_index):\n",
    "    print(f\"Document {i + 1}: {sims}\")\n",
    "\n",
    "\n",
    "# Calculate cosine similarity for TF-IDF representation\n",
    "tfidf_similarity_index = similarities.SparseMatrixSimilarity(tfidf_corpus, num_features=len(tfidf_dictionary))\n",
    "\n",
    "# Display cosine similarity scores for TF-IDF representation\n",
    "print(\"\\nCosine Similarity (TF-IDF Representation):\")\n",
    "for i, sims in enumerate(tfidf_similarity_index):\n",
    "    print(f\"Document {i + 1}: {sims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Eucliadian for bow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Similarity Matrix (bow representation):\n",
      "[[1.         0.41421357 0.36602541 0.36602541]\n",
      " [0.41421357 1.         0.5        0.36602541]\n",
      " [0.36602541 0.5        1.         0.33333333]\n",
      " [0.36602541 0.36602541 0.33333333 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_euclidean_similarity(corpus):\n",
    "    num_terms = max(token_id for doc in corpus for token_id, _ in doc) + 1\n",
    "    matrix = [sparse2full(doc, num_terms) for doc in corpus]\n",
    "    \n",
    "    # Calculate Euclidean similarity\n",
    "    num_docs = len(matrix)\n",
    "    similarity_matrix = np.zeros((num_docs, num_docs))\n",
    "    for i, doc1 in enumerate(matrix):\n",
    "        for j, doc2 in enumerate(matrix):\n",
    "            similarity_matrix[i, j] = 1 / (1 + euclidean(doc1, doc2))\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Calculate Euclidean similarity bow representation\n",
    "euclidean_similarity_bow = calculate_euclidean_similarity(bow_corpus)\n",
    "print(\"Euclidean Similarity Matrix (bow representation):\")\n",
    "print(euclidean_similarity_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Jaccard for TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
