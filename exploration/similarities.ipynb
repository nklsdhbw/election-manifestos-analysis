{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Information**\n",
    "\n",
    "Prefer spacy instead of nltk because it's faster and we want to compute large texts.\n",
    "Lemmatization and Stemming are redundand and different aproaches. They normaly are not used together. Decided for Lemmatizing because its integrated into spacy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Das ist ein Text.\",\n",
    "    \"Ich habe einen Text geschrieben.\",\n",
    "    \"Ich habe mehrere Texte geschrieben!\",\n",
    "]\n",
    "\n",
    "sp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Documents:\n",
      "['Das', 'ist', 'ein', 'Text', '.']\n",
      "['Ich', 'habe', 'einen', 'Text', 'geschrieben', '.']\n",
      "['Ich', 'habe', 'mehrere', 'Texte', 'geschrieben', '!']\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = []\n",
    "for doc in documents:\n",
    "    doc_tokens = sp(doc)\n",
    "    tokenized_tokens = [token.text for token in doc_tokens]\n",
    "    tokenized_docs.append(tokenized_tokens)\n",
    "\n",
    "print(\"Tokenized Documents:\")\n",
    "for doc_tokens in tokenized_docs:\n",
    "    print(doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lammatize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents:\n",
      "['der', 'sein', 'ein', 'Text', '--']\n",
      "['ich', 'haben', 'ein', 'Text', 'schreiben', '--']\n",
      "['ich', 'haben', 'mehrere', 'Text', 'schreiben', '--']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs = []\n",
    "for doc_tokens in tokenized_docs:\n",
    "    lemmatized_tokens = [token.lemma_ for token in sp(' '.join(doc_tokens))]\n",
    "    lemmatized_docs.append(lemmatized_tokens)\n",
    "\n",
    "print(\"\\nLemmatized Documents:\")\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Remove Stop Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Documents without Stop Words:\n",
      "['Text', '--']\n",
      "['Text', 'schreiben', '--']\n",
      "['mehrere', 'Text', 'schreiben', '--']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_docs_no_stopwords = []\n",
    "\n",
    "for lemmatized_tokens in lemmatized_docs:\n",
    "    lemmatized_tokens_no_stopwords = [token for token in lemmatized_tokens if not sp.vocab[token].is_stop]\n",
    "    lemmatized_docs_no_stopwords.append(lemmatized_tokens_no_stopwords)\n",
    "\n",
    "print(\"\\nLemmatized Documents without Stop Words:\")\n",
    "for lemmatized_tokens in lemmatized_docs_no_stopwords:\n",
    "    print(lemmatized_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
